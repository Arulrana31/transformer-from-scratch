# transformer-from-scratch

A modular, educational implementation of transformer layers, attention mechanisms, and neural networks in pure Python and NumPy.

## Features

- Self-contained attention and transformer layers
- Custom neural network with backpropagation and Adam optimizer
- Layer normalization with forward and backward passes
- Various activation and loss functions
- No external dependencies beyond NumPy

## Project Structure

- `Attention.py` - Implements attention heads, multi-head attention, and backpropagation
- `Network_Re.py` - Fully connected neural network with custom initialization and Adam updates
- `Layernorm.py` - Layer normalization with forward and backward passes
- `Functions.py` - Activation and loss functions
- `Layers.py` - Individual neural network layers
- `Transformer_Layer.py` - Composable transformer block

## Requirements

- Python 3.8+
- NumPy

## Getting Started

1. **Clone the repository:**
git clone [https://github.com/yourusername/numpy-transformer-attention.git](https://github.com/Arulrana31/transformer-from-scratch.git)
cd numpy-transformer-attention

2. **Install dependencies:**
pip install numpy


## Contributing

Pull requests and issues are welcome! Please open an issue for bug reports or feature requests.

## License

MIT License

---

**Credits:**  
Developed by [Your Name]. Inspired by classic transformer architectures.

